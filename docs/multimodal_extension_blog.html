<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SmartTurn Multimodal Architecture: Beyond Audio-Only Endpointing</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f9f9f9;
        }

        h1,
        h2,
        h3 {
            color: #2c3e50;
            margin-top: 1.5em;
        }

        h1 {
            font-size: 2.5em;
            border-bottom: 2px solid #eaeaea;
            padding-bottom: 10px;
        }

        .abstract {
            background-color: #eef2f5;
            padding: 20px;
            border-left: 5px solid #3498db;
            font-style: italic;
            margin-bottom: 30px;
        }

        code {
            background-color: #f0f0f0;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
        }

        pre {
            background-color: #272822;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }

        .diagram-placeholder {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            border: 1px dashed #ccc;
            background-color: #fff;
        }

        .caption {
            font-size: 0.9em;
            color: #777;
            text-align: center;
            margin-top: 5px;
        }

        .equation {
            background: #fff;
            padding: 15px;
            text-align: center;
            border: 1px solid #eee;
            margin: 20px 0;
        }
    </style>
</head>

<body>

    <h1>SmartTurn Multimodal: Beyond Audio-Only Endpointing</h1>

    <div class="abstract">
        <strong>Abstract:</strong> Traditional Voice Activity Detection (VAD) relies heavily on silence duration to
        determine when a user has finished speaking. This often leads to interruptions during thoughtful pauses or
        awkward latencies. In this post, we dissect the architecture of <strong>SmartTurn Multimodal</strong>, a
        late-fusion extension of OpenAI's Whisper that incorporates visual cues (mouth movement, gaze) to predict
        turn-completion with higher accuracy and lower latency.
    </div>

    <h2>1. The Problem: The "Silence Ambiguity"</h2>
    <p>
        In purely audio-based systems, silence is ambiguous. A 500ms pause could mean the user is thinking (holding the
        floor) or waiting for a response (yielding the floor). Without visual context, models must rely on conservative
        timeout thresholds (often >1000ms), which destroys the fluidity of conversation.
    </p>
    <p>
        Humans solve this by looking. An open mouth or averted gaze signals "I'm not done," even in silence. Our
        architecture aims to replicate this intuition by fusing a 3D ResNet visual encoder with a Whisper audio
        backbone.
    </p>

    <h2>2. High-Level Architecture</h2>
    <p>
        SmartTurn Multimodal uses a <strong>Late Fusion</strong> architecture. We process the audio and video modalities
        in separate, specialized streams (encoders) and merge them only at the final bottleneck layer. This preserves
        the pre-trained feature extraction capabilities of the audio expert (Whisper) while allowing a new video expert
        to modulate the final decision.
    </p>

    <div class="diagram-placeholder">
        <div class="mermaid">
            flowchart LR
                subgraph Inputs
                    A["Audio Waveform<br/>(8 seconds)"] -->|Log-Mel Spec| B("Whisper Encoder")
                    C["Video Frames<br/>(Last 32 frames)"] -->|"Resize 112x112"| D("Video Encoder<br/>R3D-18")
                end
        
                subgraph FeatureExtraction ["Feature Extraction"]
                    B -->|Context Pooling| E["Audio Embedding<br/>384-dim"]
                    D -->|Linear Projection| F["Video Embedding<br/>256-dim"]
                end
        
                subgraph LateFusion ["Late Fusion"]
                    E --> G{"Concat"}
                    F --> G
                    G -->|"Fused Vector<br/>640-dim"| H["Fusion Layer<br/>(Linear + GELU)"]
                end
        
                subgraph Output
                    H -->|Project back to 384| I["Classifier"]
                    I -->|Sigmoid| J(("Turn End<br/>Probability"))
                end
        
                style B fill:#e1f5fe,stroke:#01579b,stroke-width:2px
                style D fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
                style H fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
                style J fill:#fce4ec,stroke:#880e4f,stroke-width:2px
        </div>
        <div class="caption">Figure 1: The SmartTurn Multimodal Architecture. Audio (Whisper) and Video (R3D-18)
            branches converge at a learnable Fusion Layer.</div>
    </div>

    <h2>3. Component Deep Dive</h2>

    <h3>3.1 The "Ear": Whisper Audio Encoder</h3>
    <p>
        We utilize the encoder portion of <strong>Whisper Tiny</strong>. It processes 8 seconds of audio converted into
        80-channel log-mel spectrograms.
    </p>
    <ul>
        <li><strong>Input:</strong> \( X_{audio} \in \mathbb{R}^{80 \times 400} \) (80 Mel bands, ~400 time steps).</li>
        <li><strong>Backbone:</strong> Transformer Encoder (pre-trained on 680k hours of audio).</li>
        <li><strong>Pooling:</strong> Instead of using the final hidden state or mean pooling, we use a
            <strong>Cross-Attention Pooling</strong> mechanism (defined in SmartTurn v3) to extract a single context
            vector \( \mathbf{e}_{a} \) representing the entire utterance.</li>
    </ul>

    <h3>3.2 The "Eye": R3D-18 Video Encoder</h3>
    <p>
        For visual features, we employ a <strong>3D ResNet-18 (R3D-18)</strong>, pre-trained on the Kinetics-400
        dataset. Unlike standard 2D CNNs, 3D convolutions capture spatiotemporal features, allowing the model to
        distinguish between a "static open mouth" and "mouth closing."
    </p>
    <ul>
        <li><strong>Input:</strong> \( X_{video} \in \mathbb{R}^{3 \times 32 \times 112 \times 112} \) (3 channels, 32
            frames, 112x112 resolution). This represents roughly the last 1 second of context.</li>
        <li><strong>Transformation:</strong> The raw 512-dimensional output is projected down to a compatible latent
            space.</li>
    </ul>

    <div class="diagram-placeholder">
        <div class="mermaid">
            flowchart TD
                Input["Input Tensor<br/>(Batch, 3, 32, 112, 112)"] -->|"3D Conv"| L1["Layer 1<br/>Spatiotemporal Features"]
                L1 -->|"ResNet Blocks"| L3["R3D-18 Backbone"]
                L3 -->|"AvgPool3D"| L4["Raw Features<br/>(Batch, 512)"]
                L4 -->|"Linear Projection"| Output["Video Embedding<br/>(Batch, 256)"]
        
                style Input fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
                style Output fill:#fff3e0,stroke:#ff6f00,stroke-width:2px
        </div>
        <div class="caption">Figure 2: The R3D-18 Video Encoder processes spatiotemporal features (motion over time).
        </div>
    </div>

    <h2>4. Mathematical Formulation: The Fusion Layer</h2>
    <p>
        The core innovation lies in the fusion layer. We employ a concatenation-based late fusion strategy. Let \(
        \mathcal{F}_{audio} \) and \( \mathcal{F}_{video} \) be our encoder functions.
    </p>

    <div class="equation">
        $$ \mathbf{e}_{a} = \text{Pool}(\mathcal{F}_{audio}(X_{a})) \in \mathbb{R}^{384} $$
        $$ \mathbf{e}_{v} = \text{Linear}(\mathcal{F}_{video}(X_{v})) \in \mathbb{R}^{256} $$
    </div>

    <p>
        If video is missing (audio-only inference), we substitute \( \mathbf{e}_{v} \) with a zero tensor \( \mathbf{0}
        \). This effectively enables "modality dropout," making the model robust to camera failures.
    </p>

    <p>
        The embeddings are concatenated and projected back to the audio dimension size to satisfy the pre-trained
        classifier requirements:
    </p>

    <div class="equation">
        $$ \mathbf{h}_{fused} = \text{GELU}(\text{LayerNorm}(W_{fuse}[\mathbf{e}_{a} ; \mathbf{e}_{v}] + b_{fuse})) $$
    </div>

    <p>
        Where \( W_{fuse} \in \mathbb{R}^{384 \times (384+256)} \). The final probability \( \hat{y} \) of a turn-end
        is:
    </p>

    <div class="equation">
        $$ \hat{y} = \sigma(\text{Classifier}(\mathbf{h}_{fused})) $$
    </div>

    <div class="diagram-placeholder">
        <div class="mermaid">
            flowchart TD
                A["Audio Embedding<br/>e_a: 384-dim"] 
                B["Video Embedding<br/>e_v: 256-dim"] 
                M{"Missing Video?"}
        
                M -- Yes --> Z["Zero Tensor<br/>0: 256-dim"]
                M -- No --> B
                
                A --> C{"Concatenate"}
                Z -.-> C
                B --> C
                
                C -->|"Combined: 640-dim"| D["Linear Layer<br/>640 -> 384"]
                D --> E["Layer Norm"]
                E --> F["GELU Activation"]
                F -->|"h_fused: 384-dim"| G["To Classifier"]
        
                style C fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
                style G fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
                style Z stroke-dasharray: 5 5
        </div>
        <div class="caption">Figure 3: The Fusion Mechanism. Concatenation allows the video features to explicitly
            modulate the audio confidence.</div>
    </div>

    <h2>5. Training Dynamics: The "Student-Teacher" Approach</h2>
    <p>
        We employ a two-stage training process to prevent "Catastrophic Forgetting" of the audio cues.
    </p>

    <h3>Stage 1: Alignment (Video Student)</h3>
    <p>
        We freeze the Audio Encoder and the Classifier. Only the <strong>Video Encoder</strong> and the <strong>Fusion
            Layer</strong> are trainable.
    </p>
    <ul>
        <li><strong>Goal:</strong> The video branch must learn to output embeddings that, when added to the audio
            embeddings, push the prediction in the correct direction.</li>
        <li><strong>Intuition:</strong> If Whisper says "Ambiguous (0.5)" but the video shows a closed mouth, the video
            branch learns to output a vector that pushes the sum to "Complete (0.9)".</li>
    </ul>

    <h3>Stage 2: Joint Fine-Tuning</h3>
    <p>
        We unfreeze the entire network (Audio + Video + Fusion) and train with a low learning rate. This allows the
        audio branch to adapt slightly to the presence of visual context (e.g., relying less on silence duration)
        without losing its core ASR capabilities.
    </p>

    <h2>6. Conclusion</h2>
    <p>
        By treating video not as a separate task but as a contextual modulator for audio, SmartTurn Multimodal achieves
        robust endpointing. The architecture allows for flexible deployment: if the camera is off, the model naturally
        falls back to high-quality audio endpointing (via zero-padding), but when video is present, it gains the
        "intuition" needed for fluid human-AI interaction.
    </p>

</body>

</html>